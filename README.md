# Introduction to Statistical Learning with Python 

This repository provides Python implementations of the labs from the book **Introduction to Statistical Learning**

- Python-based implementations of all ISLR labs.
- Clear and structured examples for each machine learning method.
- Includes data loading, manipulation, visualization, and analysis.
- Practical exercises for deeper understanding.

### Chapter 1: Introduction
-  **Lab 1.1**: Setting Up Your Python Environment  
  A quick guide to setting up Python, installing necessary libraries, and preparing for statistical analysis.

### Chapter 2: Statistical Learning
-  **Lab 2.1**: Introduction to Python for Statistical Learning  
  Topics covered:
  - Basic commands and Python syntax.
  - Introduction to NumPy for numerical computing.
  - Data visualization techniques.
  - Data indexing, loading, and slicing.
  - Loops, summaries, and descriptive statistics.

### Chapter 3: Linear Regression
-  **Lab 3.1**: Simple Linear Regression  
  Learn to fit and interpret a simple linear regression model.
-  **Lab 3.2**: Multiple Linear Regression  
  Explore regression models with multiple predictors.
-  **Lab 3.3**: Interaction Terms and Non-linear Transformations  
  Incorporate interaction terms and apply non-linear transformations.
-  **Lab 3.4**: Qualitative Predictors  
  Handle categorical variables in regression models.

### Chapter 4: Classification
-  **Lab 4.1**: Logistic Regression  
  Understand the logistic model and its applications.
-  **Lab 4.2**: Linear Discriminant Analysis (LDA)  
  Explore LDA for classification tasks.
-  **Lab 4.3**: Quadratic Discriminant Analysis (QDA)  
  Learn about QDA and its differences from LDA.
-  **Lab 4.4**: K-Nearest Neighbors (KNN)  
  Implement and evaluate KNN for classification.

### Chapter 5: Resampling Methods
-  **Lab 5.1**: Cross-Validation  
  Apply validation set, LOOCV, and k-fold cross-validation methods.
-  **Lab 5.2**: The Bootstrap Method  
  Estimate variability and confidence intervals using bootstrap techniques.

### Chapter 6: Linear Model Selection and Regularization
-  **Lab 6.1**: Subset Selection Methods  
  Perform best subset selection and stepwise regression.
-  **Lab 6.2**: Ridge Regression and Lasso  
  Explore shrinkage methods to improve model performance.
-  **Lab 6.3**: Principal Component Regression (PCR) and Partial Least Squares (PLS)  
  Reduce dimensionality while maintaining predictive power.

### Chapter 7: Moving Beyond Linearity
-  **Lab 7.1**: Polynomial Regression and Step Functions  
  Model non-linear relationships with polynomial terms and step functions.
-  **Lab 7.2**: Splines and Smoothing Techniques  
  Use regression splines and smoothing splines for flexible modeling.
-  **Lab 7.3**: Generalized Additive Models (GAMs)  
  Learn GAMs for regression and classification problems.

### Chapter 8: Tree-Based Methods
-  **Lab 8.1**: Decision Trees  
  Fit and interpret regression and classification trees.
-  **Lab 8.2**: Bagging and Random Forests  
  Apply ensemble methods to improve model accuracy.
-  **Lab 8.3**: Boosting Techniques  
  Learn boosting algorithms for regression and classification.

### Chapter 9: Support Vector Machines
-  **Lab 9.1**: Support Vector Classifiers and Machines  
  Implement SVMs for linear and non-linear classification.
-  **Lab 9.2**: Multi-Class Classification  
  Extend SVMs to handle multiple classes.

### Chapter 10: Deep Learning
-  **Lab 10.1**: Neural Networks  
  Build and train single-layer and multi-layer neural networks.
-  **Lab 10.2**: Convolutional Neural Networks (CNNs)  
  Apply CNNs for image data.
-  **Lab 10.3**: Recurrent Neural Networks (RNNs)  
  Use RNNs for sequential data and time series.

### Chapter 11: Survival Analysis
-  **Lab 11.1**: Kaplan-Meier and Cox Proportional Hazards Models  
  Analyze survival data and handle censored observations.

### Chapter 12: Unsupervised Learning
-  **Lab 12.1**: Principal Component Analysis (PCA)  
  Reduce data dimensionality and explore clustering.
-  **Lab 12.2**: K-Means and Hierarchical Clustering  
  Cluster observations into meaningful groups.
